{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "## Knowledge Recommendation Processing System\n",
    "    \n",
    "### Spelix Inc. R&D Center\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,os\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "def data_preprocessing(path='./inputdata/index_model.csv'):\n",
    "    \n",
    "    xy = np.loadtxt(path, delimiter=',', dtype=np.float32)\n",
    "    X_data = xy[:, 1:-1]\n",
    "    y_data2 = xy[:, [-1]]\n",
    "    y_data_temp,y_data=[],[]\n",
    "    \n",
    "    for i in range(len(y_data2)):\n",
    "        temp=[]\n",
    "        if y_data2[i][0] < 5: temp.append(y_data2[i][0])\n",
    "        else : temp.append(5)\n",
    "        y_data_temp.append(temp)\n",
    "\n",
    "    y_data = np.asarray(y_data_temp,dtype=np.float32)\n",
    "        \n",
    "    x_train_data, x_test,  y_train_data, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "    x_train, x_valid,  y_train, y_valid = train_test_split(x_train_data, y_train_data)\n",
    "    \n",
    "    nb_classes = len(np.unique(y_data))\n",
    "    x_colum = X_data.shape[1]\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,x_valid,y_valid,nb_classes,x_colum,X_data,y_data,y_data2\n",
    "\n",
    "def sigma(x):\n",
    "    # sigmoid function\n",
    "    # σ(x) = 1 / (1 + exp(-x))\n",
    "    return 1. / (1. + tf.exp(-x))\n",
    "\n",
    "def sigma_prime(x):\n",
    "    # derivative of the sigmoid function\n",
    "    # σ'(x) = σ(x) * (1 - σ(x))\n",
    "    return sigma(x) * (1. - sigma(x))\n",
    "\n",
    "def data_embedding(nb_classes,x_colum):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, x_colum])\n",
    "    y = tf.placeholder(tf.int32, [None, 1])\n",
    "\n",
    "    target = tf.one_hot(y, nb_classes)\n",
    "    target = tf.reshape(target, [-1, nb_classes])\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    \n",
    "    Y_one_hot = tf.one_hot(y, nb_classes)  \n",
    "    Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "    \n",
    "    return X, y, target,Y_one_hot\n",
    "\n",
    "def layer_structed(X, y, target, nb_classes, x_colum):\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", shape=[x_colum, x_colum],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([x_colum]), name='bias1')\n",
    "    l1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    l1 = tf.nn.dropout(l1, keep_prob=keep_prob)\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", shape=[x_colum, x_colum],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([x_colum]), name='bias2')\n",
    "    l2 = tf.sigmoid(tf.matmul(l1, W2) + b2)\n",
    "    l2 = tf.nn.dropout(l2, keep_prob=keep_prob)\n",
    "\n",
    "    W3 = tf.get_variable(\"W3\", shape=[x_colum, x_colum],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([x_colum]), name='bias3')\n",
    "    l3 = tf.sigmoid(tf.matmul(l2, W3) + b3)\n",
    "    l3 = tf.nn.dropout(l3, keep_prob=keep_prob)\n",
    "\n",
    "    W4 = tf.get_variable(\"W4\", shape=[x_colum, x_colum],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([x_colum]), name='bias4')\n",
    "    l4 = tf.sigmoid(tf.matmul(l3, W4) + b4)\n",
    "    l4 = tf.nn.dropout(l2, keep_prob=keep_prob)\n",
    "\n",
    "    W5 = tf.get_variable(\"W5\", shape=[x_colum, nb_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([nb_classes]), name='bias5')\n",
    "    #     y_pred = tf.sigmoid(tf.matmul(l4, W5) + b5)\n",
    "    \n",
    "    # Forward propagtion\n",
    "    layer_5 = tf.matmul(X, W5) + b5\n",
    "    y_pred = sigma(layer_5)\n",
    "    \n",
    "    return W5, b5, layer_5, y_pred, keep_prob\n",
    "\n",
    "def loss_function(target,y_pred):\n",
    "    # Loss Function (end of forwad propagation)\n",
    "    loss_i = - target * tf.log(y_pred) - (1. - target) * tf.log(1. - y_pred)\n",
    "    loss = tf.reduce_mean(loss_i)\n",
    "    # Dimension Check\n",
    "    assert y_pred.shape.as_list() == target.shape.as_list()\n",
    "    return loss\n",
    "\n",
    "def optimizer(y_pred,target,layer_5,X):\n",
    "    # Back prop\n",
    "    d_loss = (y_pred - target) / (y_pred * (1. - y_pred) + 1e-7)\n",
    "    d_sigma = sigma_prime(layer_5)\n",
    "    d_b = d_loss * d_sigma #d_layer\n",
    "    d_W = tf.matmul(tf.transpose(X), d_b)\n",
    "    return d_b, d_W\n",
    "\n",
    "def pred_to_list(pred):\n",
    "    pred_list=[]\n",
    "    for i in range(len(pred)):\n",
    "        temp=[]\n",
    "        temp.append(pred[i])\n",
    "        pred_list.append(temp)\n",
    "    return pred_list\n",
    "\n",
    "def pred_by_restore(checkpoint_path,W5, b5, layer_5, y_pred, keep_prob, Y_one_hot,X_data,X,y):\n",
    "    \n",
    "    predict_list=[]\n",
    "    \n",
    "    #hypothesis\n",
    "    hypothesis = tf.nn.sigmoid(tf.matmul(X, W5) + b5)\n",
    "    \n",
    "    #prediction\n",
    "    prediction = tf.argmax(hypothesis, 1) \n",
    "    \n",
    "    #sess\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    #restore\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "    pred = sess.run(prediction, feed_dict={ X : X_data})\n",
    "    pred_list = sess.run(hypothesis,  feed_dict={ X: X_data, y: pred_to_list(pred)}).tolist()\n",
    "\n",
    "    for i in range(len(pred_list)):\n",
    "        temp=[]\n",
    "        pred_list_sort, pred_list_index = sorted(pred_list[i],reverse=True),[]\n",
    "        \n",
    "        for j in range(len(pred_list[i])):\n",
    "            pred_list_index.append(pred_list[i].index(pred_list_sort[j]))\n",
    "            \n",
    "        temp.append(pred_list_sort)\n",
    "        temp.append(pred_list_index)\n",
    "        predict_list.append(temp)\n",
    "        \n",
    "    return predict_list\n",
    "\n",
    "def hyun(y,model_0,model_1,model_2,model_3,model_4):\n",
    "    \n",
    "    final_rank=[]\n",
    "    \n",
    "    for k in range(1):\n",
    "        k=400\n",
    "        final_rank_temp, final_temp=[], []\n",
    "        \n",
    "        model_index=[model_0[k][1],model_1[k][1],model_2[k][1],model_3[k][1],model_4[k][1]]\n",
    "        model_pers=[model_0[k][0],model_1[k][0],model_2[k][0],model_3[k][0],model_4[k][0]]\n",
    "        print(model_index)\n",
    "        print(model_pers)\n",
    "        \n",
    "        for i in range(len(model_index)):\n",
    "            if i != (len(model_index)-1):\n",
    "                for j in range(6):\n",
    "                    temp=[]\n",
    "                    if model_index[i][j] != 5:\n",
    "                        temp.append((i*5)+j)\n",
    "                        temp.append(model_pers[i][j])\n",
    "                        final_temp.append(temp)\n",
    "                    else:\n",
    "                        if i==0:rr=j\n",
    "                        break\n",
    "            else:\n",
    "                for j in range(6):\n",
    "                    temp=[]\n",
    "                    temp.append((i*5)+j)\n",
    "                    temp.append(model_pers[i][j])\n",
    "                    final_temp.append(temp)\n",
    "            print(final_temp)\n",
    "                \n",
    "        final_rank_temp.append(final_temp[0][0])\n",
    "        print(final_temp[0][0])\n",
    "        final_rank_temp.append(final_temp[:6])\n",
    "        print(final_temp[:6])\n",
    "        final_rank.append(final_rank_temp)\n",
    "    print(final_rank)\n",
    "    \n",
    "    return final_rank\n",
    "\n",
    "def hyun2(model_0,model_1,model_2,model_3,model_4):\n",
    "    model_list=[]\n",
    "    for i in range(len(model_0)):\n",
    "        model_list_temp=[]\n",
    "        frist_intserrup=0\n",
    "        for j0 in range(6):\n",
    "            if model_0[i][1][j0] == 5 :\n",
    "                frist_intserrup=j0\n",
    "                break\n",
    "            else :model_list_temp.append(model_0[i][1][j0])\n",
    "        for j1 in range(6):\n",
    "            if model_1[i][1][j1] == 5 :break\n",
    "            else :model_list_temp.append(model_1[i][1][j1]+5)\n",
    "        for j2 in range(6):\n",
    "            if model_2[i][1][j2] == 5 :break\n",
    "            else :model_list_temp.append(model_2[i][1][j2]+10)\n",
    "        for j3 in range(6):\n",
    "            if model_3[i][1][j3] == 5 :break\n",
    "            else :model_list_temp.append(model_3[i][1][j3]+15)\n",
    "        for j4 in range(6):\n",
    "            if model_4[i][0][j4] < 0.5 :\n",
    "                for j5 in range(6-len(model_list_temp)):\n",
    "                    try :\n",
    "                        model_list_temp.append(model_0[i][1][frist_intserrup+j5+1])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            else : model_list_temp.append(model_4[i][1][j4]+20)\n",
    "        model_list_temp=model_list_temp[:6]\n",
    "        model_list.append(model_list_temp)\n",
    "    return model_list\n",
    "\n",
    "def save_csv(path,y_data,final_rank):\n",
    "    \n",
    "    pre_list=[]\n",
    "    bool_list=[]\n",
    "    \n",
    "    for i in range(len(final_rank)):\n",
    "        pre_list.append(final_rank[i])\n",
    "        bool_list.append(final_rank[i] == y_data.flatten()[i])\n",
    "\n",
    "    my_dict = {\"Y\": y_data.flatten(), \"Pre\": pre_list, \"c\": bool_list}\n",
    "    df = pd.DataFrame(my_dict)\n",
    "    \n",
    "    df.to_csv(path, encoding='euc-kr')\n",
    "    \n",
    "def save_csv2(path,y_data,final_rank):\n",
    "    \n",
    "    new_df = pd.DataFrame(columns=['rank_0', 'rank_1','rank_2','rank_3','rank_4','rank_5'],\n",
    "                          data=final_rank)\n",
    "    \n",
    "    y_data2=[]\n",
    "    for i in range(len(y_data)):\n",
    "        y_data2.append(y_data[i][0])\n",
    "    \n",
    "    new_df['real_Y'] = y_data2\n",
    "    \n",
    "    new_df['bool_result'] = (new_df['rank_0'] == new_df['real_Y']) | (new_df['rank_1'] == new_df['real_Y']) | (new_df['rank_2'] == new_df['real_Y']) | (new_df['rank_3'] == new_df['real_Y'])  | (new_df['rank_4'] == new_df['real_Y'])\n",
    "    \n",
    "    new_df.to_csv(path, encoding='euc-kr', index=False)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def print_predict(df):\n",
    "    \n",
    "    a=len(df.bool_result)\n",
    "    b=len(df[df['bool_result']==True])\n",
    "    c=(int(b)/int(a))*100\n",
    "    t1,t2=[],[]\n",
    "    \n",
    "    for i in range(5):\n",
    "        t1.append(len(df[df['real_Y']==df[('rank_{0}'.format(i))]]))\n",
    "\n",
    "    for i in range(5):\n",
    "        t2.append((int(t1[i])/int(a))*100)\n",
    "              \n",
    "    print(\"result_count:\", a)\n",
    "    print(\"----------------------\")\n",
    "    print(\"result_ture_count:\", b)\n",
    "    print(\"----------------------\")\n",
    "    print(\"acc(%):\", c)\n",
    "              \n",
    "    for i in range(5):\n",
    "        print('-----------------------')\n",
    "        print('rank_{0}_acc : {1} %' .format(i,t2[i]))\n",
    "    \n",
    "def train(x_train,y_train,x_test,y_test,x_valid,y_valid,nb_classes,x_colum): #back_propagtion\n",
    "    \n",
    "    learning_rate = 0.0000005\n",
    "    global_step = 500001\n",
    "    valid_step = 10001\n",
    "    view_step = 5000\n",
    "    saver_step = 10000\n",
    "    \n",
    "    #data_embedding\n",
    "    X, y, target,Y_one_hot = data_embedding(nb_classes,x_colum)\n",
    "    \n",
    "    #layer_structed\n",
    "    W5, b5, layer_5, y_pred, keep_prob = layer_structed(X, y, target, nb_classes, x_colum)\n",
    "    \n",
    "    #loss_function\n",
    "    loss = loss_function(target,y_pred)\n",
    "\n",
    "    #optimizer\n",
    "    d_b, d_W = optimizer(y_pred, target, layer_5, X)\n",
    "    \n",
    "    # Train\n",
    "    # Updating network using gradients\n",
    "    train_step = [\n",
    "        tf.assign(W5, W5 - learning_rate * d_W),\n",
    "        tf.assign(b5, b5 - learning_rate * tf.reduce_sum(d_b)),]\n",
    "\n",
    "    # Prediction and Accuracy\n",
    "    prediction = tf.argmax(y_pred, 1)\n",
    "    acct_mat = tf.equal(tf.argmax(y_pred, 1), tf.argmax(target, 1))\n",
    "    acct_res = tf.reduce_mean(tf.cast(acct_mat, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_epoch=[[global_step, x_train, y_train],\n",
    "                [valid_step, x_valid, y_valid]]\n",
    "    \n",
    "    for i in range(len(train_epoch)) :\n",
    "        for step in range(train_epoch[i][0]):\n",
    "            sess.run(train_step, feed_dict={X: train_epoch[i][1], y: train_epoch[i][2], keep_prob: 0.6})\n",
    "\n",
    "            if step % view_step == 0:\n",
    "                step_loss, acc = sess.run([loss, acct_res], feed_dict={X:x_test, y:y_test})\n",
    "                print(\"Step: {:5}\\t Loss: {:10.5f}\\t Acc: {:.2%}\" .format(step, step_loss, acc))\n",
    "            if step % saver_step == 0:\n",
    "                saver.save(sess, './model_test/', global_step=step)\n",
    "\n",
    "    pred = sess.run(prediction, feed_dict={X: x_test, keep_prob: 1})\n",
    "\n",
    "def load(nb_classes,x_colum,X_data,y_data,y_data2,path):\n",
    "\n",
    "    #data_embedding\n",
    "    X, y, target,Y_one_hot = data_embedding(nb_classes,x_colum)\n",
    "    \n",
    "    #layer_structed\n",
    "    W5, b5, layer_5, y_pred, keep_prob = layer_structed(X, y, target, nb_classes, x_colum)\n",
    "    \n",
    "    model_0 = pred_by_restore('./model/model_0',W5, b5, layer_5, y_pred, keep_prob, Y_one_hot,X_data,X,y)\n",
    "    model_1 = pred_by_restore('./model/model_1',W5, b5, layer_5, y_pred, keep_prob, Y_one_hot,X_data,X,y)\n",
    "    model_2 = pred_by_restore('./model/model_2',W5, b5, layer_5, y_pred, keep_prob, Y_one_hot,X_data,X,y)\n",
    "    model_3 = pred_by_restore('./model/model_3',W5, b5, layer_5, y_pred, keep_prob, Y_one_hot,X_data,X,y)\n",
    "    model_4 = pred_by_restore('./model/model_4',W5, b5, layer_5, y_pred, keep_prob, Y_one_hot,X_data,X,y)\n",
    "    \n",
    "    model_list = hyun2(model_0,model_1,model_2,model_3,model_4)\n",
    "    \n",
    "    new_df = save_csv2(path,y_data2,model_list)\n",
    "    \n",
    "    print_predict(new_df)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre_processing\n",
    "\n",
    "x_train,y_train,x_test,y_test,x_valid,y_valid,nb_classes,x_colum,X_data,y_data,y_data2=data_preprocessing(\n",
    "    path = './input_data/index_model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train\n",
    "\n",
    "# train(x_train,y_train,x_test,y_test,x_valid,y_valid,nb_classes,x_colum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre_Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hyun\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-641fe6238e5c>:63: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Hyun\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_0\\-10000\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_1\\-10000\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_2\\-10000\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_3\\-10000\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_4\\-10000\n",
      "result_count: 1410\n",
      "----------------------\n",
      "result_ture_count: 1309\n",
      "----------------------\n",
      "acc(%): 92.8368794326241\n",
      "-----------------------\n",
      "rank_0_acc : 70.92198581560284 %\n",
      "-----------------------\n",
      "rank_1_acc : 14.822695035460992 %\n",
      "-----------------------\n",
      "rank_2_acc : 5.177304964539007 %\n",
      "-----------------------\n",
      "rank_3_acc : 1.276595744680851 %\n",
      "-----------------------\n",
      "rank_4_acc : 0.6382978723404255 %\n"
     ]
    }
   ],
   "source": [
    "#Load\n",
    "\n",
    "new_df=load(nb_classes,x_colum,X_data,y_data,y_data2,\n",
    "     path='./predict_result/result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
